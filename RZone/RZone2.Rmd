---
title: "R Zone 2: Chapter 3, 4 and 7"
author: " Brandon Hufstetler, Garrett Alarcon, Jinan Andrews, Anson Cheng, Nick Forrest, and Nestor Herandez"
date: "8 July 2019"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
  hmtl_document: default
geometry: margin = 1in
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyfoot[CO,CE]{Forrest - Page \thepage}
fontsize: 11pt
fontfamily: mathpazo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
```

# Chapter 3
### READ IN THE CHURN DATA SET
```{r}
  setwd("~/IMGT680")
  churn <- read.csv(file = "churn.txt", stringsAsFactors=TRUE)
  
  # Show the first ten records
  churn[1:10,]
  
  # Summarize the Churn variable
  sum.churn <- summary(churn$Churn)
  sum.churn
  
  # Calculate proportion of churners
  prop.churn <- sum(churn$Churn == "True") / length(churn$Churn)
  prop.churn
```
#### Read in the curn variable in the Churn data set, look at the first ten records, see the number of true and false churns and the proportion of churners

### BAR CHART OF VARIABLE CHURN
```{r}
  barplot(sum.churn, ylim = c(0, 3000),
          main = "Bar Graph of Churners and Non-Churners",
          col = "lightblue")
  box(which = "plot", lty = "solid", col="black")
```
#### Bar graph of churners and non-churners show that close to 3000 people did not churn and around 500 people did

### MAKE A TABLE FOR COUNTS OF CHURN AND INTERNATIONAL PLAN
```{r}
  counts <- table(churn$Churn, churn$Int.l.Plan, dnn=c("Churn", "International Plan"))
  counts
```
#### Now a table is created to look at the number of people that are and are not on an international plan and whether or not they churned. We see 2664 people are not on an international plan and do not churn but 346 do churn. For those with an international plan, 186 do not churn and 137 do.

### OVERLAYED BAR CHART
```{r}
  barplot(counts, legend = rownames(counts), col = c("blue", "red"),
          ylim = c(0, 3300), ylab = "Count", xlab = "International Plan",
          main = "Comparison Bar Chart: Churn Proportions by International Plan")
  box(which = "plot", lty = "solid", col="black")
```
#### Overlayed bar chart allows us to visually see proportion of churners and non-churners by international plan

### CREATE A TABLE WITH SUMS FOR BOTH VARIABLES
```{r}
  sumtable <- addmargins(counts, FUN = sum)
  sumtable
```
#### Sumtable tells us that 3010 are not on the international plan, 323 are on the international plan, 2850 people do not churn overall and 483 people churn overall

### CREATE A TABLE OF PROPORTIONS OVER ROWS
```{r}
  row.margin <- round(prop.table(counts, margin = 1), 4)*100
  row.margin
```
#### Creating proporations over rows, we see that 93.47% of non churners are not on the international plan compared to the 6.53% of non churners on the international plan. For churners, 71.64% were not on the international plan compared to the 28.36% on the international plan

### CREATE A TABLE OF PROPORTIONS OVER COLUMNS
```{r}
  col.margin <- round(prop.table(counts, margin = 2), 4)*100
  col.margin
```  
#### For proportions by columns, we see that 88.50% of those not on the international plan do not churn compared to the 11.50% not on the international plan that churn. 57.59% of those on the international plan do not churn and 42.41% on the international plan churn.

### CLUSTERED BAR CHART, WITH LEGEND
```{r}
  barplot(counts, col = c("blue", "red"), ylim = c(0, 3300),
          ylab = "Count", xlab = "International Plan",
          main = "Churn Count by International Plan", beside = TRUE)
  legend("topright", c(rownames(counts)), col = c("blue", "red"),
         pch = 15, title = "Churn")
  box(which = "plot", lty = "solid", col="black")
```
#### Instead of an overlayed barchart, we break up the proportions of those who do and those who don't churn by international plan

### CLUSTERED BAR CHART OF CHURN AND INTERNATIONAL PLAN WITH LEGEND
```{r}
  barplot(t(counts), col = c("blue", "green"), ylim = c(0, 3300),
          ylab = "Counts", xlab = "Churn",
          main = "International Plan Count by Churn", beside = TRUE)
  legend("topright", c(rownames(counts)), col = c("blue", "green"),
         pch = 15, title = "Int'l Plan")
  box(which = "plot", lty = "solid", col="black")
```
#### Now we look at international plan count by whether customers churned or not

### HISTOGRAM OF NON-OVERLAYED CUSTOMER SERVICE CALLS
```{r}
  hist(churn$CustServ.Calls, xlim = c(0,10),
       col = "lightblue", ylab = "Count", xlab = "Customer Service Calls",
       main = "Histogram of Customer Service Calls")
```
#### This output creates a histogram of customer service calls, counting number of occurences, up to 10 customer service calls

### DOWNLOAD AND INSTALL THE R PACKAGE GGPLOT2
```{r}
  #install.packages("ggplot2")
  # Pick any CRAN mirror
  # (see example image)
  # Open the new package
  library(ggplot2)            
  
  # OVERLAYED BAR CHARTS
  ggplot() +
    geom_bar(data = churn,
             aes(x = factor(churn$CustServ.Calls),
                 fill = factor(churn$Churn)),
             position = "stack") +
    scale_x_discrete("Customer Service Calls") +
    scale_y_continuous("Percent") +
    guides(fill=guide_legend(title="Churn")) +
    scale_fill_manual(values=c("blue", "red"))
  
  ggplot() +
    geom_bar(data=churn,
             aes(x = factor(churn$CustServ.Calls),
                 fill = factor(churn$Churn)),
             position = "fill") +
    scale_x_discrete("Customer Service Calls") +
    scale_y_continuous("Percent") +
    guides(fill=guide_legend(title="Churn")) +
    scale_fill_manual(values=c("blue", "red"))
```
#### Two overlayed bar graphs. Seems like a type in the first graph as the y axis isn't percent but rather number of occurences of each number of customer service calls. The bar is split to visually see proportion of churners and non churners. The second graph changes the y axis to a percentage so instead of graphing number of occurences, the percentage of churners and non churners in each number of customer service calls can be seen.

### TWO-SAMPLE T-TEST FOR INT'L CALLS
```{r}
  # Partition data
  churn.false <- subset(churn, churn$Churn == "False.")
  churn.true <- subset(churn, churn$Churn == "True.")
  
  # Run the test
  t.test(churn.false$Intl.Calls, churn.true$Intl.Calls)
```
### two sample t-test performed for the difference in mean number of international calls for churners and non-churners is seen to be statistically significant.The p-value of 0.003186 tells us so. The confidence interval does not contain 0 either. That means the variable, international calls, is useful for predicting churn.  

### SCATTERPLOT OF EVENING MINUTES AND DAY MINUTES, COLORED BY CHURN
```{r}
  plot(churn$Eve.Mins, churn$Day.Mins,
       xlim = c(0, 400), ylim = c(0, 400),
       xlab = "Evening Minutes", ylab = "Day Minutes",
       main = "Scatterplot of Day and Evening Minutes by Churn",
       col = ifelse(churn$Churn== "True", "red", "blue"))
  legend("topright", c("True", "False"),
         col = c("red", "blue"), pch = 1, title = "Churn")
```
#### Looking at the scatterplot of day minutes vs evening minutes, the univariate evidence for a high churn rate for high evening minutes is not conclusive due. Just looking at the graph, it is difficult to come up with a definite conclusion as those who churn can't even be seen.

### SCATTERPLOT OF DAY MINUTES AND CUSTOMER SERVICE CALLS, COLORED BY CHURN
```{r}
  plot(churn$Day.Mins, churn$CustServ.Calls, xlim = c(0, 400),
       xlab = "Day Minutes", ylab = "Customer Service Calls",
       main = "Scatterplot of Day Minutes and Customer Service Calls by Churn",
       col = ifelse(churn$Churn=="True", "red", "blue"),
       pch = ifelse(churn$Churn=="True", 16, 20))
  legend("topright", c("True", "False"),
         col = c("red", "blue"), pch = c(16, 20), title = "Churn")
```
### In the graph of customer service calls versus day minutes, there appears to be an interaction between the variables  as there is a pattern in the graph.

### SCATTERPLOT MATRIX
```{r}
  pairs(~churn$Day.Mins + churn$Day.Calls + churn$Day.Charge)
```
#### We want to uncover possible correlation among the predictors. There does not seem to be any relationship between day mins and day calls, or between day calls and day charge. The scatterplots do not have a pattern to them. 

### REGRESSION OF DAY CHARGE VS DAY MINUTES
```{r}
  fit <- lm(churn$Day.Charge ~ churn$Day.Mins)
  summary(fit)
```
#### The graphical results do not support a general sanity check that there should be a correlation.Results tell us the day charge quals 0.0006134 + 0.17 times day minutes. The R squared and adjusted R squared are both 1 so the regression analysis tells us that there is a perfect linear relationship. This also tells us that we should eliminate one of these two variables from our analysis as they are perfectly correlated. 

### CORRELATION VALUES, WITH P-VALUES
```{r}
  days <- cbind(churn$Day.Mins, churn$Day.Calls, churn$Day.Charge)
  MinsCallsTest <- cor.test(churn$Day.Mins, churn$Day.Calls)
  MinsChargeTest <- cor.test(churn$Day.Mins, churn$Day.Charge)
  CallsChargeTest <- cor.test(churn$Day.Calls, churn$Day.Charge)
  round(cor(days), 4)
  MinsCallsTest$p.value
  MinsChargeTest$p.value
  CallsChargeTest$p.value
```
#### Correlation matrix between day mins, day calls and day charge is made. Of the three p values outputted, Mins ChargeTest has a small p-value of 0 and a correlation coefficient of 1, telling us that day mins and day charge are positively correlated. 

### CORRELATION VALUES AND P-VALUES IN MATRIX FORM
```{r}
  # Collect variables of interest
  corrdata <-
    cbind(churn$Account.Length, churn$VMail.Message, churn$Day.Mins, churn$Day.Calls, churn$CustServ.Calls)
  
  # Declare the matrix
  corrpvalues <- matrix(rep(0, 25), ncol = 5)
  
  # Fill the matrix with correlations
  for (i in 1:4) {
    for (j in (i+1):5) {
      corrpvalues[i,j] <-
        corrpvalues[j,i] <-
        round(cor.test(corrdata[,i],
                       corrdata[,j])$p.value,
              4)
    }
  }
  round(cor(corrdata), 4)
  corrpvalues
```
#### Correlation matrix done again. Ignoring the diagonals in the p-value matrix, The only low value (below 0.05) is 0.0264, which is the correlation between account length and day calls. The correlation is 0.0385 telling us that account length and day calls are positively correlated. 

# Chapter 4
  
### READ IN THE HOUSES DATASET AND PREPARE THE DATA
```{r}
  setwd("~/IMGT680")
  houses <- read.csv(file="houses.csv", stringsAsFactors = FALSE, header = FALSE)
  names(houses) <- c("MVAL", "MINC", "HAGE", "ROOMS", "BEDRMS", "POPN", "HHLDS", "LAT", "LONG")
  
  # Standardize the variables
  houses$MINC_Z <- (houses$MINC - mean(houses$MINC))/(sd(houses$MINC))
  houses$HAGE_Z <- (houses$HAGE - mean(houses$HAGE))/(sd(houses$HAGE))
  houses$ROOMS_Z <- (houses$ROOMS - mean(houses$ROOMS))/(sd(houses$ROOMS))
  houses$BEDRMS_Z <- (houses$BEDRMS - mean(houses$BEDRMS))/(sd(houses$BEDRMS))
  houses$POPN_Z <- (houses$POPN - mean(houses$POPN))/(sd(houses$POPN))
  houses$HHLDS_Z <- (houses$HHLDS - mean(houses$HHLDS))/(sd(houses$HHLDS))
  houses$LAT_Z <- (houses$LAT - mean(houses$LAT))/(sd(houses$LAT))
  houses$LONG_Z <- (houses$LONG - mean(houses$LONG))/(sd(houses$LONG))
  
  # Randomly select 90% for the Training dataset
  choose <- runif(dim(houses)[1],0, 1)
  test.house <- houses[which(choose < .1),]
  train.house <- houses[which(choose <= .1),]
```

### PRINCIPAL COMPONENT ANALYSIS
```{r}
  # Requires library "psych"
  #install.packages("psych")
  library(psych)
  pca1 <- principal(train.house[,c(10:17)], nfactors=8, rotate="none", scores=TRUE)
```
#### Performing PCA on part of our training data set. Extracting 8 componenets. 


### PCA RESULTS
```{r}
  # Eigenvalues:
  pca1$values
  
  # Loadings matrix,
  # variance explained,
  pca1$loadings
```
#### Eigenvalues outputted from PCA are outputted. Loadings are also outputted for each principal component. The second table tells us how much of the total variance each component explains. So for example, the first principal componenet explains 49% of the total variance. 

### SCREE PLOT
```{r}
  plot(pca1$values, type = "b", main = "Scree Plot for Houses Data")
```
#### Scree plot of the eigenvalues against the component number. This plot is helpful in looking at the upper bound of componenets to retain. In this case, it appears we should extract a maximum number of four components as after that, the line begins to straighten out. 

### PLOT FACTOR SCORES
```{r}
  pairs(~train.house$MINC + train.house$HAGE+pca1$scores[,3],
         labels = c("Median Income", "Housing Median Age", "Component 3 Scores"))
  
  pairs(~train.house$MINC + train.house$HAGE+pca1$scores[,4],
         labels = c("Median Income", "Housing Median Age", "Component 4 Scores"))
```
#### Investigate the relationship between principal components 3 and 4 and their constituent variables. We see the relationships among median income, housing median age, and the factor scores for component 3 while the second matrix plot displays the relationships among median income, housing median age, and the factor scores for componenet 4. We see between componenet 3 and median income there is a positive correlation which reflects the positive correlation of 0.922. However, it is harder to estimate the correlation between component 3 and housing median age as seen in the lack of pattern in the graphs. In the second grpah, we see a positive correlation between componenet 4 and housing median age. Therefore, we can tell that the component weight of -0.407 for housing median age in component 3 is not of practical significance and similarly for the component weight for median income in component 4. 

### CALCULATE COMMUNALITIES
```{r}
  comm3 <- loadings(pca1)[2,1]^2 + loadings(pca1)[2,2]^2 + loadings(pca1)[2,3]^2
  comm4 <- loadings(pca1)[2,1]^2 + loadings(pca1)[2,2]^2+ loadings(pca1)[2,3]^2 + loadings(pca1)[2,4]^2
  comm3; comm4
```
###Communality values for housing median age are calculated with one retaining 3 and the other retaining 4 componenets. Anything less than 0.5 is considered low as the variables share less than half of its variability in common with other variables. We see here that extracting three componenets is not enough as housing median age only shares 35% of its variance with other variables. 

### VALIDATION OF THE PRINCIPAL COMPONENTS
```{r}
  pca2 <- principal(test.house[,c(10:17)], nfactors=4, rotate="none", scores=TRUE)
  pca2$loadings
```

### READ IN AND PREPARE DATA FOR FACTOR ANALYSIS
```{r}
  setwd("~/IMGT680")
  adult <- read.csv(file="adult.txt", stringsAsFactors = FALSE)
  adult$"capnet"<- adult$capital.gain-adult$capital.loss
  adult.s <- adult[,c(1,3,5,13,16)]

  # Standardize the data:
  adult.s$AGE_Z <- (adult.s$age - mean(adult.s$age))/(sd(adult.s$age))
  adult.s$DEM_Z <- (adult.s$demogweight - mean(adult.s$demogweight))/(sd(adult.s$demogweight))
  adult.s$EDUC_Z <- (adult.s$education.num - mean(adult.s$education.num))/(sd(adult.s$education.num))
  adult.s$CAPNET_Z <- (adult.s$capnet - mean(adult.s$capnet))/(sd(adult.s$capnet))
  adult.s$HOURS_Z <- (adult.s$hours.per.week - mean(adult.s$hours.per.week))/(sd(adult.s$hours.per.week))
  
  # Randomly select a Training dataset
  choose <- runif(dim(adult.s)[1],0, 1)
  test.adult <- adult.s[which(choose < .1), c(6:10)]
  train.adult <- adult.s[which(choose >= .1), c(6:10)]
```

### BARTLETT'S TEST FOR SPHERICITY
```{r}
  # Requires package psych
  library(psych)
  corrmat1 <- cor(train.adult, method = "pearson")
  cortest.bartlett(corrmat1, n = dim(train.adult)[1])
```

### FACTOR ANALYSIS WITH FIVE COMPONENTS
```{r}
  # Requires psych, GPArotation
  #install.packages("GPArotation")
  library(GPArotation)
  fa1 <- fa(train.adult, nfactors=5, fm = "pa", rotate="none", SMC=FALSE)
  fa1$values # Eigenvalues
  fa1$loadings # Loadings, proportion of variance, and cumulative variance
```

### FACTOR ANALYSIS WITH TWO COMPONENTS
```{r}
  fa2 <- fa(train.adult, nfactors=2, fm = "pa", max.iter = 200, rotate="none")
  fa2$values # Eigenvalues
  fa2$loadings # Loadings
  fa2$communality # Communality
```

### VARIMAX ROTATION
```{r}
  fa2v <- fa(train.adult, nfactors = 2, fm = "pa", max.iter = 200, rotate="varimax")
  fa2v$loadings
  fa2v$communality
```

### USER-DEFINED COMPOSITES
```{r}
  small.houses <- houses[,c(4:7)]
  a <- c(1/4, 1/4, 1/4, 1/4)
  W <- t(a)*small.houses
```

# Chapter 7
  
### READ IN THE DATA, PARTITION TRAINING AND TESTING DATA
```{r}
  setwd("~/IMGT680")
  adult <- read.csv(file = "adult.txt", stringsAsFactors=TRUE)
  choose <- runif(length(adult$income), min = 0, max = 1)
  training <- adult[choose <= 0.75,]
  testing <- adult[choose > 0.75,]
  adult[1:5, c(1,2,3)]
  training[1:5, c(1,2,3)]
  testing[1:5, c(1,2,3)] 
```
#### Reading in the adult data and taking in length of their income. Partitioning if length is less than or equal to 0.75 for training and if greater, put into testing data set.

### REMOVE THE TARGET VARIABLE, INCOME, FROM THE TESTING DATA
```{r}
  names(testing)
  
  # Target variable is in Column 15
  testing <- testing[,-15]
  names(testing)
  # Target variable is no longer in the testing data
```
#### Here, the variable income is removed from the testing data set.The -15 is because income is in the 15th column

### REMOVE THE PARTITIONING VARIABLE, PART, FROM BOTH DATA SETS
```{r}
  # Part is now the 15th variable
  testing <- testing[,-15]
  names(testing)
  names(training)
  
  # Part is the 16th variable in the training data set
  training <- training[,-16]
  names(training)
```  
#### Remove the partitioning variable Part from both data sets. -15 indicates that Part is currently the 15th variable and it is being removed in the testing and the 16th in training. 